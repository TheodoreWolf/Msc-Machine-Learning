{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYs6LMEbNqoQ"
   },
   "source": [
    "# RL coursework, part IV (30 pts in total)\n",
    "\n",
    "---\n",
    "\n",
    "**Name:** Your Name\n",
    "\n",
    "**SN:** Your Student Number\n",
    "\n",
    "---\n",
    "\n",
    "**Due date:** *22nd March, 2022,*\n",
    "\n",
    "---\n",
    "\n",
    "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
    "\n",
    "## How to submit\n",
    "\n",
    "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part4.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNuohp44N00i"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "#### Q.1: You will implement a number of off-policy multi-step return estimates, and answer questions about their accuracy.\n",
    "\n",
    "#### Q.2: You will be looking at other, TD-like, updates to learn the value function. You will be asked to investigate different properties of these: e.g. convergence properties, variance of updates. This is akin to a typical analysis one would undertaken when proposing a new update rule to learn value functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1p0fpbxQLyn"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=1)\n",
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thswfgXU_p05"
   },
   "source": [
    "## Q1 [11 points total]\n",
    "For many concrete algorithms, we need to combine multi-step updates with off-policy corrections.  The multi-step updates are necessary for efficient learning, while the off-policy corrections are necessary to learn about multiple things at once, or to correct for a distribution mismatch (e.g., when trying to perform a policy-gradient update from logged data).\n",
    "\n",
    "In this section, you will implement various different returns with off-policy corrections.  The next cell has two examples *without* corrections.  These examples compute equivalent returns, but compute those returns in different ways.  These are provided as reference implementations to help you.\n",
    "\n",
    "Note that the implementations both allow for immediate bootstrapping on the current state value. This is unconventional (most literature only allows the first bootstrapping to happen after the first step), but we will use this convention in all implementations below for consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHNH35SZYHBu"
   },
   "outputs": [],
   "source": [
    "#@title On-policy return computations\n",
    "\n",
    "def on_policy(observations, actions, pi, mu, rewards, discounts,\n",
    "              trace_parameter, v_fn):\n",
    "  \"\"\"Compute on-policy return recursively.\"\"\"\n",
    "  del mu  # The policy probabilities are ignored by this function\n",
    "  del pi\n",
    "  T = len(rewards)  # number of transitions\n",
    "  r = rewards\n",
    "  d = discounts\n",
    "  l = trace_parameter\n",
    "  v = np.array([v_fn(o) for o in observations])\n",
    "  G = np.zeros((T,))\n",
    "  # recurse backwards to calculate returns\n",
    "  for t in reversed(range(T)):\n",
    "    # There are T+1 observations, but only T rewards, and the indexing here\n",
    "    # for the rewards is off by one compared to the indexing in the slides\n",
    "    # and in Sutton & Barto.  In other words, r[t] == R_{t+1}.\n",
    "    if t == T - 1:\n",
    "      G[t] = r[t] + d[t]*v[t + 1]\n",
    "    else:\n",
    "      G[t] = r[t] + d[t]*((1 - l)*v[t + 1] + l*G[t + 1])\n",
    "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
    "  return (1 - l)*v + l*G\n",
    "\n",
    "def on_policy_error_recursion(observations, actions, pi, mu, rewards, discounts,\n",
    "                              trace_parameter, v_fn):\n",
    "  del pi  # The target policy probabilities are ignored by this function\n",
    "  del mu  # The behaviour policy probabilities are ignored by this function\n",
    "  T = len(rewards)  # number of transitions\n",
    "  r = rewards\n",
    "  d = discounts\n",
    "  l = trace_parameter\n",
    "  v = np.array([v_fn(o) for o in observations])\n",
    "  errors = np.zeros((T,))\n",
    "    \n",
    "  error = 0.\n",
    "  # recurse backwards to calculate errors\n",
    "  for t in reversed(range(T)):\n",
    "    error = r[t] + d[t]*v[t + 1] - v[t] + d[t]*l*error\n",
    "    errors[t] = error\n",
    "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
    "  return v + l*errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNXhobrYHeiy"
   },
   "source": [
    "### Q 1.1 [5 points]\n",
    "Implement the return functions below and run the cells below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPzHHrmn5Tm7"
   },
   "outputs": [],
   "source": [
    "def full_importance_sampling(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
    "  \"\"\"\n",
    "  Compute off-policy return with full importance-sampling corrections, so that\n",
    "  the return G_t is corrected with the full importance-sampling correction of\n",
    "  the rest of the trajectory.\n",
    "  \"\"\"\n",
    "\n",
    "def per_decision(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
    "  \"\"\"\n",
    "  Compute off-policy return with per-decision importance-sampling corrections.\n",
    "  \"\"\"\n",
    "\n",
    "def control_variates(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
    "  \"\"\"\n",
    "  Compute off-policy return with \n",
    "  1. per-decision importance-sampling corrections, and\n",
    "  2. control variates\n",
    "  \"\"\"\n",
    "\n",
    "def adaptive_bootstrapping(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
    "  \"\"\"\n",
    "  Compute off-policy return with \n",
    "  1. per-decision importance-sampling corrections, and\n",
    "  2. control variates, and\n",
    "  3. adaptive bootstrapping.\n",
    "\n",
    "  Implement the adaptive bootstrapping with an *additional* trace parameter\n",
    "  lambda, such that lambda_t = lambda * min(1, 1/rho_t).\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EEHYK89ANIA"
   },
   "outputs": [],
   "source": [
    "#@title (Run, don't modify) Functions to generate experience, compute values\n",
    "MU_RIGHT = 0.5\n",
    "PI_RIGHT = 0.9\n",
    "NUMBER_OF_STEPS = 5\n",
    "DISCOUNT = 0.99\n",
    "\n",
    "def generate_experience():\n",
    "  r\"\"\"Generate experience trajectories from a tabular tree MDP.\n",
    "\n",
    "  This function will start in state 0, and will then generate actions according\n",
    "  to a uniformly random behaviour policy.  When A_t == 0, the action will be to\n",
    "  the left, with A_t==1, it will be to the right.  The states are nunmbered as\n",
    "  depicted below:\n",
    "          0\n",
    "         / \\\n",
    "        1   2\n",
    "       / \\ / \\\n",
    "      3   4   5\n",
    "         ...\n",
    "  \n",
    "  Args:\n",
    "      number_of_steps: the number of total steps.\n",
    "      p_right: probability of the behaviour to go right.\n",
    "\n",
    "  Returns:\n",
    "      A dictionary with elements:\n",
    "        * observations (number_of_steps + 1 integers): the\n",
    "          observations are just the actual (integer) states\n",
    "        * actions (number_of_steps integers): actions per step\n",
    "        * rewards (number_of_steps scalars): rewards per step\n",
    "        * discounts (number_of_steps scalars): currently always 0.9,\n",
    "          except the last one which is zero\n",
    "        * mu (number_of_steps scalars): probability of selecting each\n",
    "          action according to the behavious policy\n",
    "        * pi (number_of_steps scalars): probability of selecting each\n",
    "          action according to the target policy (here p(1) = 0.9 and\n",
    "          p(0) = 0.1, where a==1 implies we go 'right')\n",
    "  \"\"\"\n",
    "  # generate actions\n",
    "  actions = np.array(np.random.random(NUMBER_OF_STEPS,) < MU_RIGHT,\n",
    "                     dtype=np.int)\n",
    "  s = 0\n",
    "  # compute resulting states\n",
    "  states = np.cumsum(np.arange(1, NUMBER_OF_STEPS + 1) + actions)\n",
    "  states = np.array([0] + list(states))  # add start state\n",
    "\n",
    "  # in this case, observations are just the real states\n",
    "  observations = states\n",
    "\n",
    "  # generate rewards\n",
    "  rewards     = 2.*actions - 1. # -1 for left, +1 for right, \n",
    "  rewards[-1] = np.sum(actions)  # extra final reward for going right\n",
    "    \n",
    "  # compute discounts\n",
    "  discounts     = DISCOUNT * np.ones_like(rewards)\n",
    "  discounts[-1] = 0.  # final transition is terminal, has discount=0\n",
    "\n",
    "  # determine target and behaviour probabilities for the selected actions\n",
    "  pi = np.array([1. - PI_RIGHT, PI_RIGHT])[actions] # Target probabilities\n",
    "  mu = np.array([1. - MU_RIGHT, MU_RIGHT])[actions] # Behaviour probabilities\n",
    "    \n",
    "  return dict(observations=observations,\n",
    "              actions=actions,\n",
    "              pi=pi,\n",
    "              mu=mu,\n",
    "              rewards=rewards,\n",
    "              discounts=discounts)\n",
    "\n",
    "def true_v(s, pi, number_of_steps):\n",
    "  \"\"\"Compute true state value recursively.\"\"\"\n",
    "  depth = int(np.floor((np.sqrt(1 + 8*s) - 1)/2))\n",
    "  position = int(s - depth*(depth+1)/2)\n",
    "  remaining_steps = number_of_steps - depth\n",
    "  final_reward = DISCOUNT**(remaining_steps-1)*(position + pi*remaining_steps)\n",
    "  reward_per_step = pi*(+1) + (1 - pi)*(-1)\n",
    "  discounted_steps = (1 - DISCOUNT**(remaining_steps - 1))/(1 - DISCOUNT)\n",
    "  reward_along_the_way = reward_per_step * discounted_steps\n",
    "  return reward_along_the_way + final_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCFMUmOfRTqZ"
   },
   "outputs": [],
   "source": [
    "#@title Run experiment (don't modify)\n",
    "algs = ['on_policy', 'full_importance_sampling', 'per_decision', 'control_variates', 'adaptive_bootstrapping']\n",
    "\n",
    "# Precompute state values (for efficiency)\n",
    "N = NUMBER_OF_STEPS\n",
    "true_vs = [true_v(s, PI_RIGHT, N) for s in range((N+1)*(N+2)//2)]\n",
    "\n",
    "def random_v(iteration, s):\n",
    "  rng = np.random.RandomState(seed=s + iteration*10000)\n",
    "  return true_vs[s] + rng.normal(loc=0, scale=1.)  # Add fixed random noise \n",
    "\n",
    "def plot_errors(ax, errors):\n",
    "  errors = np.array(errors)\n",
    "  ax.violinplot(np.log10(errors), showextrema=False)\n",
    "  ax.plot(range(1, len(algs)+1), np.log10(errors).T,\n",
    "          '.', color='#667799', ms=7, alpha=0.2)\n",
    "  ax.plot(range(1, len(algs)+1), np.log10(np.mean(errors, axis=0)),\n",
    "          '.', color='#000000', ms=20)\n",
    "  ax.set_yticks(np.arange(-2, 5))\n",
    "  ax.set_yticklabels(10.**np.arange(-2, 5), fontsize=13)\n",
    "  ax.set_ylabel(\"Value error $(v(s_0) - v_{\\\\pi}(s_0))^2$\", fontsize=15)\n",
    "  ax.set_xticks(range(1, len(algs)+1))\n",
    "  ax.set_xticklabels(algs, fontsize=15, rotation=70)\n",
    "  ax.set_ylim(-1, 4)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "errors = []\n",
    "estimates = []\n",
    "v0 = true_vs[0]\n",
    "for iteration in range(1000):\n",
    "  errors.append([])\n",
    "  estimates.append([])\n",
    "  trajectory = generate_experience()\n",
    "  for alg in algs:\n",
    "    estimate = eval(alg)(**trajectory,\n",
    "                        v_fn=lambda s: random_v(iteration, s),\n",
    "                        trace_parameter=0.9)\n",
    "    errors[-1].append((estimate[0] - v0)**2)\n",
    "print(np.mean(errors, axis=0))\n",
    "plot_errors(plt.gca(), errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hlc4jctHHqv"
   },
   "source": [
    "Above, the distributions of mean squared value errors are shown, with the mean as a big black dot and the (1,000) individual return samples as small black dots.\n",
    "\n",
    "### Q 1.2 [3 points]\n",
    "Explain the ranking in terms of value error of the different return estimates.\n",
    "\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0Uk1d9b4CPc"
   },
   "source": [
    "\n",
    "\n",
    "### Q 1.3 [3 points]\n",
    "Could there be a reason to **not** choose the best return according to this ranking when learning off-policy?  Explain your answer.\n",
    "\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5Xn8bDNFm-H"
   },
   "source": [
    "## Q2 [19 points total]\n",
    " Consider a MDP $M = (\\mathcal{S}, \\mathcal{A}, p, r, \\gamma)$ and a behaviour policy $\\mu$. We use policy $\\mu$ to generate trajectories of experience:\n",
    "\\begin{equation*}\n",
    "    (s_{t}, a_{t}, r_{t},s_{t+1}, a_{t+1}, r_{t+1},\\cdots, s_{t+n-1}, a_{t+n-1}, r_{t+n-1}, s_{t+n}, a_{t+n}) \\,.\n",
    "\\end{equation*}\n",
    "Note that this is an $n$-step sequence, starting from time $t$.\n",
    "\n",
    "Given these partial trajectories we consider the following learning problems:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vc8FfT06lYY"
   },
   "source": [
    "### Q2.1 [9 points]\n",
    "Consider a learning update based on the following temporal difference error:\n",
    "$$\\delta_t = R(S_t, A_t) + \\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q(S_{t+2}, a) - q(S_t, A_t)$$\n",
    "\n",
    "Consider updating a tabular action value function with TD.\n",
    "\n",
    "i) Does the resulting value function converge, under any initialisation of the value function? Consider an appropiate learning rate (Robbins–Monro conditions). If so, prove the convergence under infinity number of interactions with this MDP, under fixed behaviour policy $\\mu$ and show its convergence point. If not, show why it diverges. (7 points)\n",
    "\n",
    "ii) Under which conditions, would the above process converge to the optimal value function $q_*$ ? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H4p8jZj6bGP"
   },
   "source": [
    "*Answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfIqVjrP6QI-"
   },
   "source": [
    "### Q2.2 [10 points]\n",
    "\n",
    "Consider the same questions now for the following temporal difference error\n",
    "\\begin{equation}\n",
    "        \\delta_t = r(S_{t},A_{t}) + \\gamma \\frac{\\pi(A_{t+1}|S_{t+1})}{\\mu(A_{t+1}|S_{t+1})} \\left[ r(S_{t+1},A_{t+1}) + \\gamma \\max_{a} q(S_{t+2},a) \\right] - q(S_t, A_t)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\pi(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$ and consider the behaviour policy to be either:\n",
    "\n",
    "  a. $\\mu(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$,\n",
    "  \n",
    "  b. $\\mu(a|s) = \\frac{1}{|\\mathcal{A}|}$ (uniformly random policy).\n",
    "\n",
    "Answer the below two questions for **both choices** of the behaviour policy $\\mu$:\n",
    "* i)  Does updating a tabular action value function with this TD error converge to the optimal value function $q_*$? Consider an appropiate learning rate (Robbins–Monro conditions). If so, prove this convergence under infinity number of interaction with this MDP, under behaviour policy $\\mu$. If not, show why it diverges or alternatively convergence to a different solution. (4 points)\n",
    "* ii) How does the variance of this update compare to the one induced by the error in Q5.1? (3 points). \n",
    "* iii) Can you propose a different behaviour policy that achieves a lower variance than any of the choices we considered for $\\mu$? Prove that your behaviour policy achieve this. Argue why, if that is not possible. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGqQJRzD6Tww"
   },
   "source": [
    "*Answer here:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wesi6S866Lyq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/dm_python:dm_notebook3",
    "kind": "private"
   },
   "name": "UCL RL assignment 2022, part IV",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1JU6mjNeZzP6ezHB8Vu5YqV-_Q9iN2CVv",
     "timestamp": 1645461208957
    },
    {
     "file_id": "1dkLVv6gagPKbfB1UCY997gOfqOXpeQD-",
     "timestamp": 1645457029049
    },
    {
     "file_id": "1zYag0A0o4ncxyHc2Uq2ESf7CEBhktnv8",
     "timestamp": 1645456932996
    },
    {
     "file_id": "1KK-ZHRlK2WjPv7etN5KHUwEAGnPSOxbI",
     "timestamp": 1645455353656
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2021/RL_assignment_3.ipynb?cl=368650897",
     "timestamp": 1618589207256
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2021/RL_assignment_3_solution.ipynb?workspaceId=mtthss:ucluclucl::citc",
     "timestamp": 1618502477520
    },
    {
     "file_id": "1q0yqRznin1Zv5-MduEZbZZU8EF-qfzxJ",
     "timestamp": 1618501430836
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
