{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6818b72",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78af22",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2aa1f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4e1b37fc90a19a5feedba370ac4d63c",
     "grade": false,
     "grade_id": "cell-51e5da9ddd4cc99c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94413c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c13f9dafa7cba64bcba61eed889fb01f",
     "grade": false,
     "grade_id": "cell-a4d703cf4d20ee02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Bayesian linear classifiers\n",
    "\n",
    "In this example you are going to fit a Bayesian logistic regression model, using two sets of features on data $\\mathbf{x} \\in \\mathbb{R}^2$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{\\text{simple}}(\\mathbf{x}) &= [ 1, x_1, x_2 ] \\\\\n",
    "\\phi_{\\text{polynomial}}(\\mathbf{x}) &= [ 1, x_1, x_2, x_1x_2, x_1^2, x_2^2, x_1^3, x_2^3 ]\n",
    "\\end{align*}$$\n",
    "\n",
    "and three types of estimation:\n",
    "\n",
    "1. MAP estimation (penalized maximum likelihood)\n",
    "2. Laplace approximation (a Gaussian approximate posterior, centered at the mode)\n",
    "3. **for extra credit**, Variational Bayes (a Gaussian approximate posterior, found by maximizing a lower bound on the model \"evidence\", i.e. the marginal likelihood)\n",
    "\n",
    "Here is a synthetic dataset that we'll be working with (plotting the training set only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ecb2d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31183244bf0b5d69adc8b367d67c7472",
     "grade": false,
     "grade_id": "cell-07ecbcc2cafce227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = torch.load(\"data.pt\")\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolors='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ca843",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bb38dc4a791cf0b433e81577abb2499",
     "grade": false,
     "grade_id": "cell-791ab24aadbc08a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here are definitions of two different feature maps, the \"simple\" one and the \"polynomial\" one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f254ff7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad469169b8195e35278932709d80ca6b",
     "grade": false,
     "grade_id": "cell-318dc4fe1089d515",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def features_simple(X):\n",
    "    return torch.concat((torch.ones_like(X[:,:1]), X), -1)\n",
    "\n",
    "def features_poly(X):\n",
    "    interactions = X.prod(-1, keepdim=True)\n",
    "    return torch.concat((torch.ones_like(X[:,:1]), \n",
    "                         X, X.pow(2), X.pow(3),\n",
    "                         interactions), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2e543",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "268279fdb7e070742cb5496dee1f8d7b",
     "grade": false,
     "grade_id": "cell-390950f2d4a30066",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK #1 (3 points): Define the model\n",
    "\n",
    "The Bayesian logistic regression model we are working with has the form\n",
    "$$\\begin{align*}\n",
    "\\mathbf{w} &\\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I}) \\\\\n",
    "\\hat y_i &= \\mathrm{Logistic}(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i)) \\\\\n",
    "y_i &\\sim \\mathrm{Bernoulli}(\\hat y_i)\n",
    "\\end{align*}$$\n",
    "where $i = 1,\\dots, N$ and the Logistic function is defined\n",
    "$$\\begin{align*}\n",
    "\\mathrm{Logistic}(z) &= \\frac{1}{1 + \\exp\\{-z\\}}.\n",
    "\\end{align*}$$\n",
    "It's implemented in pytorch as `torch.sigmoid`.\n",
    "\n",
    "The first step is to define two functions, one to make predictions given a weight vector $\\mathbf{w}$ and inputs $\\Phi$, and one which computes the log joint probability\n",
    "\n",
    "$$\\log p(\\mathbf{y}, \\mathbf{w} | \\mathbf{\\Phi}, \\sigma^2).$$\n",
    "\n",
    "I've done the first one for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477dc07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e5e5517d0441c8d5c12c738d656ab4a",
     "grade": false,
     "grade_id": "cell-2bfc837bc01b0b20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_probs_MAP(Phi, w):\n",
    "    \"\"\"\n",
    "    Given a \"design matrix\" Phi, and a point estimate w, compute p(y = 1 | Phi, w)\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    w     : (D,) vector of weights\n",
    "\n",
    "    OUTPUT:\n",
    "    y_hat : (N,) vector of probabilities p(y=1 | Phi, w)\n",
    "    \"\"\"\n",
    "    return torch.sigmoid(Phi @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fc311",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d1b81c6d59b9a0f4a84327f277287d0",
     "grade": false,
     "grade_id": "B-log-joint",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_joint(Phi, y, w, sigma=10):\n",
    "    \"\"\"\n",
    "    Compute the joint probability of the data and the latent variables.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w     : (D,) vector of weights\n",
    "    sigma : scalar, standard deviation of Gaussian prior distribution p(w).\n",
    "            Leave this set to sigma=10 for purposes of this exercise\n",
    "\n",
    "    OUTPUT:\n",
    "    log_joint : the log probability log p(y, w | Phi, sigma), a scalar\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a307f74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7414775d4662069e3ed76e8c157c3663",
     "grade": true,
     "grade_id": "B-joint-test-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478df225",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56476349ca26272aa489be1492bc6d3b",
     "grade": true,
     "grade_id": "B-joint-test-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc5535",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "592fde4a1c2412dd8c0e3bdd6f3bec09",
     "grade": false,
     "grade_id": "cell-91a0bc31c0fe7b9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK 2 (5 points): Implement MAP estimation\n",
    "\n",
    "Now you need to write a function which performs MAP estimation, i.e. penalized maximum likelihood estimation.\n",
    "\n",
    "This function should find the value $\\mathbf{w}_{MAP}$ that maximizes the log joint, i.e.\n",
    "\n",
    "$$\\mathbf{w}_{MAP} = \\mathrm{argmax}_{\\mathbf{w}}\\log p(\\mathbf{y}, \\mathbf{w} | \\mathbf{\\Phi}, \\sigma^2).$$\n",
    "\n",
    "To do this, you should **use pytorch autograd tools**. This will involve defining an initial value of the weights, computing a scalar loss function, and calling `.backward()`, and then performing gradient-based optimization. Take a look at the demo notebooks from previous lectures for examples…!\n",
    "\n",
    "* You **may feel free to use classes from `torch.optim`**. I would suggest the use of `torch.optim.SGD` or `torch.optim.Adagrad`.\n",
    "* Regardless of how you do this, you will need to decide on a stopping criteria for your optimization routine.\n",
    "* You will also need to decide on how to set the parameters (learning rate, momentum, anything else!) for your selected optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed4742",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0136517993392d94f124c530ccf7f33",
     "grade": false,
     "grade_id": "B-find-MAP",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_MAP(Phi, y):\n",
    "    \"\"\"\n",
    "    Find the MAP estimate of the log_joint method.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    w      : (D,) vector of optimized weights\n",
    "    losses : list of losses at each iteration of the optimization algorithm.\n",
    "             Should be a list of scalars, which can be plotted afterward to\n",
    "             diagnose convergence.\n",
    "    \"\"\"\n",
    "\n",
    "    weights = torch.zeros(Phi.shape[1]).requires_grad_(True)\n",
    "    losses = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return weights.detach(), losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c9cf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9db727380135210315c5bf49f0c0e9",
     "grade": false,
     "grade_id": "cell-0bb96b3160b8580d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following two cells call `find_MAP` to compute $\\mathbf{w}$ for both choices of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_MAP_simple, losses = find_MAP(features_simple(X_train), y_train)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afa5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_MAP_poly, losses = find_MAP(features_poly(X_train), y_train)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363aeaa0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d4352ba194de7f7d86b9ccc051fc9e4",
     "grade": true,
     "grade_id": "B-test-MAP-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927c5f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5505930918b72a9db67ba7e1f3f843",
     "grade": true,
     "grade_id": "B-test-MAP-2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a9a9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c088909096a75d2f55ad28f00f4b7db",
     "grade": false,
     "grade_id": "cell-f4589a3dbb0d0545",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Visualization: The following code visualizes the classifier result\n",
    "\n",
    "It plots the probability of being one class or the other using a color contour plot.\n",
    "\n",
    "The decision boundary is a dashed black line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(X, y, pred):\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = pred(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=.8, levels=np.linspace(0, 1, 8))\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    \n",
    "    plt.contour(xx, yy, Z, levels=(0.5,), linestyles='dashed');\n",
    "    \n",
    "    # Plot the testing points\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_simple(X), w_MAP_simple))\n",
    "plt.title(\"MAP estimate, simple features\")\n",
    "train_accuracy = (predict_probs_MAP(features_simple(X_train), w_MAP_simple).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_probs_MAP(features_simple(X_test), w_MAP_simple).round() == y_test).float().mean()\n",
    "print(\"Simple features: training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(X_train, y_train,\n",
    "              lambda X: predict_probs_MAP(features_poly(X), w_MAP_poly))\n",
    "plt.title(\"MAP estimate, polynomial features\")\n",
    "train_accuracy = (predict_probs_MAP(features_poly(X_train), w_MAP_poly).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_probs_MAP(features_poly(X_test), w_MAP_poly).round() == y_test).float().mean()\n",
    "print(\"Polynomial features: training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51394eb5",
   "metadata": {},
   "source": [
    "# TASK #3 (5 points): Laplace approximation\n",
    "\n",
    "In the next section, you will fit an approximate posterior over the weights by using the Laplace approximation around the mode $\\mathbf{w}_{MAP}$ of the distribution you found above.\n",
    "\n",
    "This requires completing two functions:\n",
    "\n",
    "1. `compute_laplace_Cov` takes the data and the MAP estimate, and outputs a covariance matrix defined as the negative inverse Hessian of the log target density. (See the week 4 lecture slides for details on how to compute this!)\n",
    "2. `predict_bayes` makes predictions on new data points, by approximating $\\int p(y | x, w)p(w | \\mathcal{D})dw$ when using a Gaussian approximation to $p(w | \\mathcal{D})$. In the week 4 lecture slides we discussed three different ways of computing this — it is up to you to decide what method you would prefer, and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b04a17",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b04031d87b33a98b89706e5bee51f95",
     "grade": false,
     "grade_id": "B-laplace-cov",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_laplace_Cov(Phi, y, w_MAP, prior_std=10):\n",
    "    \"\"\"\n",
    "    Compute the Laplace approximation of the posterior covariance \n",
    "    in a logistic regression setting.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    sigma : scalar, standard deviation of Gaussian prior distribution p(w).\n",
    "            Leave this set to sigma=10 for purposes of this exercise\n",
    "\n",
    "    OUTPUT:\n",
    "    Cov : (D, D) posterior covariance matrix estimate defined by the Laplace \n",
    "          approximation\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da31180",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "532ca1f2a118acd58c737ec7545cd821",
     "grade": false,
     "grade_id": "B-laplace-predict",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_bayes(Phi, w_MAP, Cov):\n",
    "    \"\"\"\n",
    "    Make predictions on new data points using an approximate posterior \n",
    "    w ~ MultivariateNormal(w_MAP, Cov)\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    Cov   : (D, D) approximate posterior covariance matrix\n",
    "    \n",
    "    OUTPUT:\n",
    "    y_hat : (N,) vector of probabilities p(y=1 | Phi)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2c239",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b20317869f5427299c73a7aed504e984",
     "grade": false,
     "grade_id": "cell-e563aca374ddf46f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following cells call your functions above to compute the Laplace approximation and visualize the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e440e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13417e655e2239c0b67ca963c6d3d70a",
     "grade": false,
     "grade_id": "cell-9d607a490ff71c7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Cov_simple = compute_laplace_Cov(features_simple(X_train), y_train, w_MAP_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Simple features: MAP\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_simple(X), w_MAP_simple))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Simple features: Laplace\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_simple(X), w_MAP_simple, Cov_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adf170",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf79a1dbbf7a8e3a70d926f2300a7ebf",
     "grade": false,
     "grade_id": "cell-e6516c087b43b9a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Cov_poly = compute_laplace_Cov(features_poly(X_train), y_train, w_MAP_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Polynomial features: MAP\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_poly(X), w_MAP_poly))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Polynomial features: Laplace\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_poly(X), w_MAP_poly, Cov_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9e528",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aebc89b362aa8eb7be81e982941cd56b",
     "grade": true,
     "grade_id": "B-test-Laplace-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f87376",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94674dc5dd062cf200627a4c8cac432e",
     "grade": true,
     "grade_id": "B-test-Laplace-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a5380",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "264d3021a237270917668394c5505e58",
     "grade": true,
     "grade_id": "B-test-Laplace-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567389d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e965d127f726030eaa6252f83ee6a35d",
     "grade": false,
     "grade_id": "cell-303212e96a305a6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK #4 (2 points): Model comparison\n",
    "\n",
    "You can compute the marginal likelihood approximation defined by the Laplace approximation.\n",
    "\n",
    "This estimate of the evidence can be used, even just looking at the training data, to help decide which of the two feature maps is more appropriate and better fits the data.\n",
    "\n",
    "This can help guard against potential overfitting if using features that are \"too complex\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dd545",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34fc8b4b795c39c977b6c55e09e2af59",
     "grade": false,
     "grade_id": "B-laplace-evidence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_laplace_log_evidence(Phi, y, w_MAP, Cov):\n",
    "    \"\"\"\n",
    "    This computes the Laplace approximation to the marginal likelihood,\n",
    "    as defined in the Week 5 lectures.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    Cov   : (D, D) approximate posterior covariance matrix\n",
    "    \n",
    "    OUTPUT:\n",
    "    log_evidence : scalar value estimating `log p(y | Phi)`\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model evidence estimate (simple features):\",\n",
    "      compute_laplace_log_evidence(features_simple(X_train), y_train, w_MAP_simple, Cov_simple).item())\n",
    "\n",
    "print(\"Model evidence estimate (polynomial features):\",\n",
    "      compute_laplace_log_evidence(features_poly(X_train), y_train, w_MAP_poly, Cov_poly).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b63d8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d355fa2808934801c1c2f480b80243d5",
     "grade": true,
     "grade_id": "B-test-evidence",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc74644",
   "metadata": {},
   "source": [
    "# EXTRA CREDIT TASK #5 (6 points): Variational Bayes\n",
    "\n",
    "For the Variational Bayes approach, we define an evidence lower bound (ELBO)\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\Phi}, \\mathbf{y};\\mathbf{\\mu}, \\mathbf{\\Sigma}) = E_{q(\\mathbf{w} | \\mathbf{\\mu}, \\mathbf{\\Sigma})}[\\log p(\\mathbf{y}, \\mathbf{w} | \\mathbf{\\Phi}) - \\log q(\\mathbf{w} | \\mathbf{\\mu}, \\mathbf{\\Sigma})]$$\n",
    "\n",
    "which we then can optimize, using gradient-based methods, to find optimal values of $\\mathbf{\\mu}^\\star, \\mathbf{\\Sigma}^\\star$ that make $$q(\\mathbf{w} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) \\approx p(\\mathbf{w} | \\mathbf{\\Phi}, \\mathbf{y}).$$\n",
    "\n",
    "To do this, you should implement\n",
    "* `ELBO`, which returns a Monte Carlo estimate of the ELBO. This is not a deterministic function! It will require sampling from the approximate posterior $q(\\mathbf{w} | \\mathbf{\\mu}, \\mathbf{\\Sigma})$.\n",
    "* `estimate_VB`, which will be similar to the `find_MAP` function you wrote in part 2 — except instead of maximizing the log joint with respect to $\\mathbf{w}$, it will maximize the ELBO with respect to $\\mathbf{\\mu}, \\mathbf{\\Sigma}$.\n",
    "\n",
    "The marking for this section is:\n",
    "* 2 points for implementing `ELBO` correctly, where $\\mathbf{\\Sigma}$ is a diagonal covariance matrix\n",
    "* 2 points for implementing `estimate_VB` correctly, where $\\mathbf{\\Sigma}$ is a diagonal covariance matrix\n",
    "* 2 bonus points if $\\mathbf{\\Sigma}$ is a full-rank $D \\times D$ covariance matrix, instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668531cd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f2f1c05a1c3e4525d25f307d31b6d09",
     "grade": false,
     "grade_id": "B-elbo",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ELBO(Phi, y, mu, Cov):\n",
    "    \"\"\"\n",
    "    Return a Monte Carlo estimate of the evidence lower bound (ELBO)\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    mu    : (D,) vector of variational parameters, indicating the \n",
    "            estimated posterior mean\n",
    "    Cov   : (D, D) matrix of variational parameters, indicating the \n",
    "            estimated posterior covariance matrix\n",
    "    \n",
    "    OUTPUT:\n",
    "    ELBO : scalar value estimating the ELBO\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ed154",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e91d9be3e72bd413a43c60765174274c",
     "grade": false,
     "grade_id": "B-estimate-vb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_VB(Phi, y, mu_init):\n",
    "    \"\"\"\n",
    "    Return a Monte Carlo estimate of the evidence lower bound (ELBO)\n",
    "    \n",
    "    INPUT:\n",
    "    Phi     : (N, D) tensor of input features, where N is the number of \n",
    "              observations and D is the number of features\n",
    "    y       : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "              containing zeros and ones\n",
    "    mu_init : (D,) initial value of the variational parameters for the\n",
    "              posterior mean.\n",
    "\n",
    "            \n",
    "    OUTPUT:\n",
    "    mu     : (D,) vector of variational parameters, indicating the \n",
    "             estimated posterior mean\n",
    "    Cov    : (D, D) matrix of variational parameters, indicating the \n",
    "             estimated posterior covariance matrix\n",
    "    losses : list of losses at each iteration of the optimization algorithm.\n",
    "             Should be a list of scalars, which can be plotted afterward to\n",
    "             diagnose convergence.\n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return mu, Cov, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d1209",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "220a192df02e1b244c701ef0b4ba01ed",
     "grade": false,
     "grade_id": "cell-48bc72cc5ea79a14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Estimate VB parameters, and plot the result (comparing to the Laplace approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_VB_poly, Cov_VB_poly, losses = estimate_VB(features_poly(X_train), y_train, mu_init=w_MAP_poly)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ab458",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Polynomial features: Laplace\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_poly(X), w_MAP_poly, Cov_poly))\n",
    "\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, marker='^', edgecolor='k');\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Polynomial features: VB\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_poly(X), w_VB_poly, Cov_VB_poly))\n",
    "\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, marker='^', edgecolor='k');\n",
    "\n",
    "\n",
    "train_accuracy = (predict_bayes(features_poly(X_train), w_VB_poly, Cov_VB_poly).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_bayes(features_poly(X_test), w_VB_poly, Cov_VB_poly).round() == y_test).float().mean()\n",
    "print(\"Polynomial features, VB: training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874171b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfec6681f7d6e756efda0c87cdd29246",
     "grade": true,
     "grade_id": "B-test-VB-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc6fa1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac026185361f58155ae5583b31cbd63a",
     "grade": true,
     "grade_id": "B-test-VB-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a063207",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b2ce96c540197ca0541cca49081ca4e",
     "grade": true,
     "grade_id": "B-test-VB-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0dab0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01295be53bcb884576e25514c5a8fb53",
     "grade": true,
     "grade_id": "B-test-VB-4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
